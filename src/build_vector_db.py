"""
æ„å»ºå‘é‡æ•°æ®åº“
ç”¨æ³•: python build_vector_db.py
"""

import os
import chromadb
from pathlib import Path
import hashlib

def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50):
    """æ–‡æœ¬åˆ†å—"""
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        if chunk.strip():
            chunks.append(chunk)
    return chunks

def build_database(docs_dir: str = "../data/raw_docs", 
                   db_path: str = "../data/vector_db"):
    """æ„å»ºå‘é‡æ•°æ®åº“"""
    
    print(f"ğŸ“š Building vector database from: {docs_dir}")
    print(f"ğŸ’¾ Database will be saved to: {db_path}")
    
    # åˆå§‹åŒ–ChromaDBï¼ˆä½¿ç”¨é»˜è®¤embeddingï¼‰
    client = chromadb.PersistentClient(path=db_path)
    
    # åˆ é™¤æ—§collectionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        client.delete_collection("nus_docs")
        print("ğŸ—‘ï¸  Deleted old collection")
    except:
        pass
    
    # åˆ›å»ºæ–°collection
    collection = client.create_collection(
        name="nus_docs",
        metadata={"description": "NUS student life documents"}
    )
    
    docs_path = Path(docs_dir)
    if not docs_path.exists():
        print(f"âŒ Directory not found: {docs_dir}")
        print(f"ğŸ“ Creating directory...")
        docs_path.mkdir(parents=True, exist_ok=True)
        return
    
    # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
    supported_extensions = ['.txt', '.md']
    all_files = []
    for ext in supported_extensions:
        all_files.extend(docs_path.glob(f'**/*{ext}'))
    
    if not all_files:
        print(f"âš ï¸  No documents found in {docs_dir}")
        print(f"ğŸ“ Please add .txt or .md files to {docs_dir}")
        return
    
    print(f"ğŸ“„ Found {len(all_files)} documents")
    
    total_chunks = 0
    
    for file_path in all_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                continue
            
            # åˆ†å—
            chunks = chunk_text(content, chunk_size=300, overlap=50)
            
            # ä¸ºæ¯ä¸ªchunkç”Ÿæˆå”¯ä¸€ID
            for idx, chunk in enumerate(chunks):
                chunk_id = hashlib.md5(
                    f"{file_path.name}_{idx}".encode()
                ).hexdigest()
                
                collection.add(
                    documents=[chunk],
                    ids=[chunk_id],
                    metadatas=[{
                        'source': file_path.name,
                        'chunk_idx': idx
                    }]
                )
                total_chunks += 1
            
            print(f"  âœ… {file_path.name}: {len(chunks)} chunks")
            
        except Exception as e:
            print(f"  âŒ Error processing {file_path.name}: {e}")
    
    print(f"\nâœ¨ Database built successfully!")
    print(f"ğŸ“Š Total chunks indexed: {total_chunks}")
    print(f"ğŸ’¾ Saved to: {db_path}")

if __name__ == "__main__":
    build_database()
