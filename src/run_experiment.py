"""
ä¸»å®éªŒè„šæœ¬
ç”¨æ³•: python run_experiment.py
"""

import json
import os
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from rag_system import RAGSystem
from evaluator import Evaluator

def load_questions(questions_dir: str = "../questions"):
    """åŠ è½½æ‰€æœ‰é—®é¢˜"""
    
    question_types = ['factual', 'procedural', 'comparative', 'recommendation']
    all_questions = {}
    
    questions_path = Path(questions_dir)
    if not questions_path.exists():
        print(f"ğŸ“ Creating questions directory: {questions_dir}")
        questions_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹é—®é¢˜æ–‡ä»¶
        examples = {
            'factual': [
                "What is the address of NUS Central Library?",
                "How many residential colleges does NUS have?",
                "What are the operating hours of MPSH gym?"
            ],
            'procedural': [
                "How do I apply for on-campus accommodation?",
                "What is the process for module registration?",
                "How do I book a study room in the library?"
            ],
            'comparative': [
                "What are the differences between Halls and Residential Colleges?",
                "Compare UTown Residence and PGPR.",
                "Compare Central Library and Science Library."
            ],
            'recommendation': [
                "Which hall should I choose if I want an active social life?",
                "I'm on a budget. What accommodation do you recommend?",
                "Where should I eat if I'm vegetarian?"
            ]
        }
        
        for qtype, questions in examples.items():
            filepath = questions_path / f"{qtype}.txt"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write('\n'.join(questions))
            print(f"  âœ… Created example file: {qtype}.txt")
        
        print(f"\nğŸ“ Please edit question files in {questions_dir}")
        print(f"   Each line = one question")
        return None
    
    # åŠ è½½é—®é¢˜
    for qtype in question_types:
        filepath = questions_path / f"{qtype}.txt"
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8') as f:
                questions = [line.strip() for line in f if line.strip()]
            all_questions[qtype] = questions
            print(f"âœ… Loaded {len(questions)} {qtype} questions")
        else:
            print(f"âš ï¸  File not found: {qtype}.txt")
            all_questions[qtype] = []
    
    total = sum(len(q) for q in all_questions.values())
    print(f"ğŸ“Š Total questions: {total}")
    
    return all_questions

def run_experiment(models: list, rag_system: RAGSystem, questions_dict: dict):
    """è¿è¡Œå®Œæ•´å®éªŒ"""
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path(f"../results/experiment_{timestamp}")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ Results will be saved to: {results_dir}")
    
    all_results = {}
    
    for model in models:
        print(f"\n{'='*80}")
        print(f"ğŸ¤– Testing model: {model}")
        print(f"{'='*80}")
        
        model_results = {}
        
        for qtype, questions in questions_dict.items():
            if not questions:
                continue
            
            print(f"\nğŸ“‹ Question type: {qtype.upper()}")
            
            type_results = []
            
            for question in tqdm(questions, desc=f"  Processing"):
                # RAGæŸ¥è¯¢
                result = rag_system.query(question, model, qtype, top_k=5)
                
                if result['success']:
                    # è¯„æµ‹
                    evaluation = Evaluator.evaluate(result['answer'], qtype)
                    
                    # åˆå¹¶ç»“æœ
                    full_result = {
                        'question': question,
                        'answer': result['answer'],
                        'retrieved_docs_count': len(result['retrieved_docs']),
                        'evaluation': evaluation
                    }
                    
                    type_results.append(full_result)
                else:
                    print(f"\n    âŒ Failed: {result.get('error', 'Unknown error')}")
            
            # è®¡ç®—è¯¥ç±»å‹å¹³å‡åˆ†
            if type_results:
                avg_score = sum(r['evaluation']['total_score'] for r in type_results) / len(type_results)
                print(f"  ğŸ“Š Average score: {avg_score:.3f}")
                
                model_results[qtype] = {
                    'questions_count': len(type_results),
                    'average_score': round(avg_score, 3),
                    'details': type_results
                }
        
        all_results[model] = model_results
        
        # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
        model_file = results_dir / f"{model.replace(':', '_')}.json"
        with open(model_file, 'w', encoding='utf-8') as f:
            json.dump(model_results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Saved: {model_file.name}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary(all_results, results_dir)
    
    return all_results

def generate_summary(all_results: dict, results_dir: Path):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    
    print(f"\n{'='*80}")
    print("ğŸ“Š EXPERIMENT SUMMARY")
    print(f"{'='*80}\n")
    
    # è¡¨æ ¼å¤´
    print(f"{'Model':<20} {'Type':<20} {'Questions':<12} {'Avg Score':<12}")
    print("-"*80)
    
    summary = {
        'timestamp': datetime.now().isoformat(),
        'models': {}
    }
    
    for model, results in all_results.items():
        summary['models'][model] = {}
        
        for qtype, data in results.items():
            avg_score = data['average_score']
            count = data['questions_count']
            
            print(f"{model:<20} {qtype:<20} {count:<12} {avg_score:<12.3f}")
            
            summary['models'][model][qtype] = {
                'questions_count': count,
                'average_score': avg_score
            }
        
        print("-"*80)
    
    # ä¿å­˜æ±‡æ€»
    summary_file = results_dir / "summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Summary saved to: {summary_file}")
    print(f"âœ¨ Experiment completed!")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ NUS RAG Experiment System")
    print("="*80)
    
    # 1. åŠ è½½é—®é¢˜
    questions_dict = load_questions()
    if questions_dict is None:
        return
    
    # 2. åˆå§‹åŒ–RAGç³»ç»Ÿ
    try:
        rag_system = RAGSystem()
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("ğŸ“ Please run: python build_vector_db.py first")
        return
    
    # 3. å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹
    models = [
        "qwen2.5:3b",
        "qwen2.5:1.5b",
        "llama3.2:3b",
        "phi3:mini",
        "deepseek-r1:1.5b"
    ]
    
    print(f"\nğŸ¤– Models to test: {', '.join(models)}")
    
    # 4. è¿è¡Œå®éªŒ
    run_experiment(models, rag_system, questions_dict)

if __name__ == "__main__":
    main()
